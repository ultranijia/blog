<!DOCTYPE html><html lang><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 使用 TensorFlow Serving 和 Docker 快速部署机器学习服务 · QIWIHUI</title><meta name="description" content="使用 TensorFlow Serving 和 Docker 快速部署机器学习服务 - qiwihui"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/nella.css"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.12/css/all.css" integrity="sha384-G0fIWCsCzJIMAVNQPfjH08cyYaUtMwjJwqiRKxxE/rx96Uroj1BtIQ6MLJuheaO9" crossorigin="anonymous"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "ca-pub-8935595858652656",
enable_page_level_ads: true
});
</script><link rel="search" type="application/opensearchdescription+xml" href="https://qiwihui.com/atom.xml" title="QIWIHUI"><script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/images/avatar.jpg" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">HOME</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li><li class="nav-list-item"><a href="/links/" target="_self" class="nav-list-link">LINKS</a></li><!--li.nav-list-item--><!--    a.nav-list-link(class="search" href=url_for("search") target="_self") SEARCH--></ul></header><main class="container"></main><div class="post"><article class="post-block"><h1 class="post-title">使用 TensorFlow Serving 和 Docker 快速部署机器学习服务</h1><div class="post-info">Mar 3, 2019<span class="categories"><i class="fa fa-bookmark" aria-hidden="true"></i></span><span>15 min. read</span></div><div class="post-content"><p>从实验到生产，简单快速部署机器学习模型一直是一个挑战。这个过程要做的就是将训练好的模型对外提供预测服务。在生产中，这个过程需要可重现，隔离和安全。这里，我们使用基于Docker的TensorFlow Serving来简单地完成这个过程。TensorFlow 从1.8版本开始支持Docker部署，包括CPU和GPU，非常方便。</p>
<h2>获得训练好的模型</h2>
<p>获取模型的第一步当然是训练一个模型，但是这不是本篇的重点，所以我们使用一个已经训练好的模型，比如ResNet。TensorFlow Serving 使用SavedModel这种格式来保存其模型，SavedModel是一种独立于语言的，可恢复，密集的序列化格式，支持使用更高级别的系统和工具来生成，使用和转换TensorFlow模型。这里我们直接下载一个预训练好的模型：</p>
<a id="more"></a>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir /tmp/resnet</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> curl -s https://storage.googleapis.com/download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v2_fp32_savedmodel_NHWC_jpg.tar.gz | tar --strip-components=2 -C /tmp/resnet -xvz</span></span><br></pre></td></tr></table></figure>
<p>如果是使用其他框架比如Keras生成的模型，则需要将模型转换为SavedModel格式，比如：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 中间省略模型构建</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型转换为SavedModel</span></span><br><span class="line">signature = tf.saved_model.signature_def_utils.predict_signature_def(</span><br><span class="line">    inputs=&#123;<span class="string">'input_param'</span>: model.input&#125;, outputs=&#123;<span class="string">'type'</span>: model.output&#125;)</span><br><span class="line">builder = tf.saved_model.builder.SavedModelBuilder(<span class="string">'/tmp/output_model_path/1/'</span>)</span><br><span class="line">builder.add_meta_graph_and_variables(</span><br><span class="line">    sess=K.get_session(),</span><br><span class="line">    tags=[tf.saved_model.tag_constants.SERVING],</span><br><span class="line">    signature_def_map=&#123;</span><br><span class="line">        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:</span><br><span class="line">            signature</span><br><span class="line">    &#125;)</span><br><span class="line">builder.save()</span><br></pre></td></tr></table></figure>
<p>下载完成后，文件目录树为：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tree /tmp/resnet</span></span><br><span class="line">/tmp/resnet</span><br><span class="line">└── 1538687457</span><br><span class="line">    ├── saved_model.pb</span><br><span class="line">    └── variables</span><br><span class="line">        ├── variables.data-00000-of-00001</span><br><span class="line">        └── variables.index</span><br></pre></td></tr></table></figure>
<h2>部署模型</h2>
<p>使用Docker部署模型服务：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker pull tensorflow/serving</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker run -p 8500:8500 -p 8501:8501 --name tfserving_resnet \</span></span><br><span class="line">--mount type=bind,source=/tmp/resnet,target=/models/resnet \</span><br><span class="line">-e MODEL_NAME=resnet -t tensorflow/serving</span><br></pre></td></tr></table></figure>
<p>其中，<code>8500</code>端口对于TensorFlow Serving提供的gRPC端口，<code>8501</code>为REST API服务端口。<code>-e MODEL_NAME=resnet</code>指出TensorFlow Serving需要加载的模型名称，这里为<code>resnet</code>。上述命令输出为</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2019-03-04 02:52:26.610387: I tensorflow_serving/model_servers/server.cc:82] Building single TensorFlow model file config:  model_name: resnet model_base_path: /models/resnet</span><br><span class="line">2019-03-04 02:52:26.618200: I tensorflow_serving/model_servers/server_core.cc:461] Adding/updating models.</span><br><span class="line">2019-03-04 02:52:26.618628: I tensorflow_serving/model_servers/server_core.cc:558]  (Re-)adding model: resnet</span><br><span class="line">2019-03-04 02:52:26.745813: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable &#123;name: resnet version: 1538687457&#125;</span><br><span class="line">2019-03-04 02:52:26.745901: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version &#123;name: resnet version: 1538687457&#125;</span><br><span class="line">2019-03-04 02:52:26.745935: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version &#123;name: resnet version: 1538687457&#125;</span><br><span class="line">2019-03-04 02:52:26.747590: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /models/resnet/1538687457</span><br><span class="line">2019-03-04 02:52:26.747705: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /models/resnet/1538687457</span><br><span class="line">2019-03-04 02:52:26.795363: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags &#123; serve &#125;</span><br><span class="line">2019-03-04 02:52:26.828614: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</span><br><span class="line">2019-03-04 02:52:26.923902: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:162] Restoring SavedModel bundle.</span><br><span class="line">2019-03-04 02:52:28.098479: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:138] Running MainOp with key saved_model_main_op on SavedModel bundle.</span><br><span class="line">2019-03-04 02:52:28.144510: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:259] SavedModel load for tags &#123; serve &#125;; Status: success. Took 1396689 microseconds.</span><br><span class="line">2019-03-04 02:52:28.146646: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:83] No warmup data file found at /models/resnet/1538687457/assets.extra/tf_serving_warmup_requests</span><br><span class="line">2019-03-04 02:52:28.168063: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version &#123;name: resnet version: 1538687457&#125;</span><br><span class="line">2019-03-04 02:52:28.174902: I tensorflow_serving/model_servers/server.cc:286] Running gRPC ModelServer at 0.0.0.0:8500 ...</span><br><span class="line">[warn] getaddrinfo: address family for nodename not supported</span><br><span class="line">2019-03-04 02:52:28.186724: I tensorflow_serving/model_servers/server.cc:302] Exporting HTTP/REST API at:localhost:8501 ...</span><br><span class="line">[evhttp_server.cc : 237] RAW: Entering the event loop ...</span><br></pre></td></tr></table></figure>
<p>我们可以看到，TensorFlow Serving使用<code>1538687457</code>作为模型的版本号。我们使用curl命令来查看一下启动的服务状态，也可以看到提供服务的模型版本以及模型状态。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl http://localhost:8501/v1/models/resnet</span></span><br><span class="line">&#123;</span><br><span class="line"> "model_version_status": [</span><br><span class="line">  &#123;</span><br><span class="line">   "version": "1538687457",</span><br><span class="line">   "state": "AVAILABLE",</span><br><span class="line">   "status": &#123;</span><br><span class="line">    "error_code": "OK",</span><br><span class="line">    "error_message": ""</span><br><span class="line">   &#125;</span><br><span class="line">  &#125;</span><br><span class="line"> ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2>查看模型输入输出</h2>
<p>很多时候我们需要查看模型的输出和输出参数的具体形式，TensorFlow提供了一个<code>saved_model_cli</code>命令来查看模型的输入和输出参数：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> saved_model_cli show --dir /tmp/resnet/1538687457/ --all</span></span><br><span class="line"></span><br><span class="line">MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:</span><br><span class="line"></span><br><span class="line">signature_def['predict']:</span><br><span class="line">  The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">    inputs['image_bytes'] tensor_info:</span><br><span class="line">        dtype: DT_STRING</span><br><span class="line">        shape: (-1)</span><br><span class="line">        name: input_tensor:0</span><br><span class="line">  The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">    outputs['classes'] tensor_info:</span><br><span class="line">        dtype: DT_INT64</span><br><span class="line">        shape: (-1)</span><br><span class="line">        name: ArgMax:0</span><br><span class="line">    outputs['probabilities'] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 1001)</span><br><span class="line">        name: softmax_tensor:0</span><br><span class="line">  Method name is: tensorflow/serving/predict</span><br><span class="line"></span><br><span class="line">signature_def['serving_default']:</span><br><span class="line">  The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">    inputs['image_bytes'] tensor_info:</span><br><span class="line">        dtype: DT_STRING</span><br><span class="line">        shape: (-1)</span><br><span class="line">        name: input_tensor:0</span><br><span class="line">  The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">    outputs['classes'] tensor_info:</span><br><span class="line">        dtype: DT_INT64</span><br><span class="line">        shape: (-1)</span><br><span class="line">        name: ArgMax:0</span><br><span class="line">    outputs['probabilities'] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 1001)</span><br><span class="line">        name: softmax_tensor:0</span><br><span class="line">  Method name is: tensorflow/serving/predict</span><br></pre></td></tr></table></figure>
<p>注意到<code>signature_def</code>，<code>inputs</code>的名称，类型和输出，这些参数在接下来的模型预测请求中需要。</p>
<h2>使用模型接口预测：REST和gRPC</h2>
<p>TensorFlow Serving提供REST API和gRPC两种请求方式，接下来将具体这两种方式。</p>
<h3>REST</h3>
<p>我们下载一个客户端脚本，这个脚本会下载一张猫的图片，同时使用这张图片来计算服务请求时间。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -o /tmp/resnet/resnet_client.py https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/example/resnet_client.py</span></span><br></pre></td></tr></table></figure>
<p>以下脚本使用<code>requests</code>库来请求接口，使用图片的base64编码字符串作为请求内容，返回图片分类，并计算了平均处理时间。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># The server URL specifies the endpoint of your server running the ResNet</span></span><br><span class="line"><span class="comment"># model with the name "resnet" and using the predict interface.</span></span><br><span class="line">SERVER_URL = <span class="string">'http://localhost:8501/v1/models/resnet:predict'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The image URL is the location of the image we should send to the server</span></span><br><span class="line">IMAGE_URL = <span class="string">'https://tensorflow.org/images/blogs/serving/cat.jpg'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="comment"># Download the image</span></span><br><span class="line">  dl_request = requests.get(IMAGE_URL, stream=<span class="literal">True</span>)</span><br><span class="line">  dl_request.raise_for_status()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Compose a JSON Predict request (send JPEG image in base64).</span></span><br><span class="line">  jpeg_bytes = base64.b64encode(dl_request.content).decode(<span class="string">'utf-8'</span>)</span><br><span class="line">  predict_request = <span class="string">'&#123;"instances" : [&#123;"b64": "%s"&#125;]&#125;'</span> % jpeg_bytes</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Send few requests to warm-up the model.</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    response = requests.post(SERVER_URL, data=predict_request)</span><br><span class="line">    response.raise_for_status()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Send few actual requests and report average latency.</span></span><br><span class="line">  total_time = <span class="number">0</span></span><br><span class="line">  num_requests = <span class="number">10</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_requests):</span><br><span class="line">    response = requests.post(SERVER_URL, data=predict_request)</span><br><span class="line">    response.raise_for_status()</span><br><span class="line">    total_time += response.elapsed.total_seconds()</span><br><span class="line">    prediction = response.json()[<span class="string">'predictions'</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">  print(<span class="string">'Prediction class: &#123;&#125;, avg latency: &#123;&#125; ms'</span>.format(</span><br><span class="line">      prediction[<span class="string">'classes'</span>], (total_time*<span class="number">1000</span>)/num_requests))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  main()</span><br></pre></td></tr></table></figure>
<p>输出结果为</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> python resnet_client.py</span></span><br><span class="line">Prediction class: 286, avg latency: 210.12310000000002 ms</span><br></pre></td></tr></table></figure>
<h3>gRPC</h3>
<p>让我们下载另一个客户端脚本，这个脚本使用gRPC作为服务，传入图片并获取输出结果。这个脚本需要安装<code>tensorflow-serving-api</code>这个库。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -o /tmp/resnet/resnet_client_grpc.py https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/example/resnet_client_grpc.py</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install tensorflow-serving-api</span></span><br></pre></td></tr></table></figure>
<p>脚本内容：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is a placeholder for a Google-internal import.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> grpc</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow_serving.apis <span class="keyword">import</span> predict_pb2</span><br><span class="line"><span class="keyword">from</span> tensorflow_serving.apis <span class="keyword">import</span> prediction_service_pb2_grpc</span><br><span class="line"></span><br><span class="line"><span class="comment"># The image URL is the location of the image we should send to the server</span></span><br><span class="line">IMAGE_URL = <span class="string">'https://tensorflow.org/images/blogs/serving/cat.jpg'</span></span><br><span class="line"></span><br><span class="line">tf.app.flags.DEFINE_string(<span class="string">'server'</span>, <span class="string">'localhost:8500'</span>,</span><br><span class="line">                           <span class="string">'PredictionService host:port'</span>)</span><br><span class="line">tf.app.flags.DEFINE_string(<span class="string">'image'</span>, <span class="string">''</span>, <span class="string">'path to image in JPEG format'</span>)</span><br><span class="line">FLAGS = tf.app.flags.FLAGS</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> FLAGS.image:</span><br><span class="line">    <span class="keyword">with</span> open(FLAGS.image, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">      data = f.read()</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># Download the image since we weren't given one</span></span><br><span class="line">    dl_request = requests.get(IMAGE_URL, stream=<span class="literal">True</span>)</span><br><span class="line">    dl_request.raise_for_status()</span><br><span class="line">    data = dl_request.content</span><br><span class="line"></span><br><span class="line">  channel = grpc.insecure_channel(FLAGS.server)</span><br><span class="line">  stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)</span><br><span class="line">  <span class="comment"># Send request</span></span><br><span class="line">  <span class="comment"># See prediction_service.proto for gRPC request/response details.</span></span><br><span class="line">  request = predict_pb2.PredictRequest()</span><br><span class="line">  request.model_spec.name = <span class="string">'resnet'</span></span><br><span class="line">  request.model_spec.signature_name = <span class="string">'serving_default'</span></span><br><span class="line">  request.inputs[<span class="string">'image_bytes'</span>].CopyFrom(</span><br><span class="line">      tf.contrib.util.make_tensor_proto(data, shape=[<span class="number">1</span>]))</span><br><span class="line">  result = stub.Predict(request, <span class="number">10.0</span>)  <span class="comment"># 10 secs timeout</span></span><br><span class="line">  print(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  tf.app.run()</span><br></pre></td></tr></table></figure>
<p>输出的结果可以看到图片的分类，概率和使用的模型信息：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> python resnet_client_grpc.py</span></span><br><span class="line">outputs &#123;</span><br><span class="line">  key: "classes"</span><br><span class="line">  value &#123;</span><br><span class="line">    dtype: DT_INT64</span><br><span class="line">    tensor_shape &#123;</span><br><span class="line">      dim &#123;</span><br><span class="line">        size: 1</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    int64_val: 286</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">outputs &#123;</span><br><span class="line">  key: "probabilities"</span><br><span class="line">  value &#123;</span><br><span class="line">    dtype: DT_FLOAT</span><br><span class="line">    tensor_shape &#123;</span><br><span class="line">      dim &#123;</span><br><span class="line">        size: 1</span><br><span class="line">      &#125;</span><br><span class="line">      dim &#123;</span><br><span class="line">        size: 1001</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    float_val: 2.4162832232832443e-06</span><br><span class="line">    float_val: 1.9012182974620373e-06</span><br><span class="line">    float_val: 2.7247710022493266e-05</span><br><span class="line">    float_val: 4.426385658007348e-07</span><br><span class="line">    ...(中间省略)</span><br><span class="line">    float_val: 1.4636580090154894e-05</span><br><span class="line">    float_val: 5.812107133351674e-07</span><br><span class="line">    float_val: 6.599806511076167e-05</span><br><span class="line">    float_val: 0.0012952701654285192</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">model_spec &#123;</span><br><span class="line">  name: "resnet"</span><br><span class="line">  version &#123;</span><br><span class="line">    value: 1538687457</span><br><span class="line">  &#125;</span><br><span class="line">  signature_name: "serving_default"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2>性能</h2>
<h3>通过编译优化的TensorFlow Serving二进制来提高性能</h3>
<p>TensorFlows serving有时会有输出如下的日志：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</span><br></pre></td></tr></table></figure>
<p>TensorFlow Serving已发布Docker镜像旨在尽可能多地使用CPU架构，因此省略了一些优化以最大限度地提高兼容性。如果你没有看到此消息，则你的二进制文件可能已针对你的CPU进行了优化。根据你的模型执行的操作，这些优化可能会对你的服务性能产生重大影响。幸运的是，编译优化的TensorFlow Serving二进制非常简单。官方已经提供了自动化脚本，分以下两部进行：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1. 编译开发版本</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker build -t <span class="variable">$USER</span>/tensorflow-serving-devel -f Dockerfile.devel https://github.com/tensorflow/serving.git<span class="comment">#:tensorflow_serving/tools/docker</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2. 生产新的镜像</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker build -t <span class="variable">$USER</span>/tensorflow-serving --build-arg TF_SERVING_BUILD_IMAGE=<span class="variable">$USER</span>/tensorflow-serving-devel https://github.com/tensorflow/serving.git<span class="comment">#:tensorflow_serving/tools/docker</span></span></span><br></pre></td></tr></table></figure>
<p>之后，使用新编译的<code>$USER/tensorflow-serving</code>重新启动服务即可。</p>
<h2>总结</h2>
<p>上面我们快速实践了使用TensorFlow Serving和Docker部署机器学习服务的过程，可以看到，TensorFlow Serving提供了非常方便和高效的模型管理，配合Docker，可以快速搭建起机器学习服务。</p>
<h2>参考</h2>
<ul>
<li><a href="https://medium.com/tensorflow/serving-ml-quickly-with-tensorflow-serving-and-docker-7df7094aa008" target="_blank" rel="noopener">Serving ML Quickly with TensorFlow Serving and Docker</a></li>
<li><a href="https://www.tensorflow.org/tfx/serving/tutorials/Serving_REST_simple" target="_blank" rel="noopener">Train and serve a TensorFlow model with TensorFlow Serving</a></li>
</ul>
<blockquote>
<p>GitHub repo: <a href="https://github.com/qiwihui/blog" target="_blank" rel="noopener">qiwihui/blog</a></p>
<p>Follow me: <a href="https://github.com/qiwihui" target="_blank" rel="noopener">@qiwihui</a></p>
<p>Site: <a href="https://qiwihui.com">QIWIHUI</a></p>
</blockquote>
</div><p class="post-tags"><i class="fa fa-tags" aria-hidden="true"></i><a href="/tags/技术/">#技术</a><a href="/tags/机器学习/">#机器学习</a></p></article></div><div class="post-copyright"><blockquote><p>版权声明：本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 | CC BY-NC-SA 4.0 </a>许可协议。</p></blockquote></div><footer><div class="paginator"><a href="/qiwihui-blog-59/" class="prev">PREV</a><a href="/qiwihui-blog-57/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'blog-qiwihui-com';
var disqus_identifier = 'qiwihui-blog-58/';
var disqus_title = '使用 TensorFlow Serving 和 Docker 快速部署机器学习服务';
var disqus_url = 'https://qiwihui.com/qiwihui-blog-58/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//blog-qiwihui-com.disqus.com/count.js" async></script><!-- block copyright--></footer></div><script async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true,
        processEnvironments: true,
        skipTags: ["script","noscript","style","textarea","code","pre"]
    }
});
</script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-46660488-3",'auto');ga('send','pageview');</script><link rel="stylesheet" href="//cdn.datatables.net/1.10.7/css/jquery.dataTables.min.css" media="screen" type="text/css"><script src="//cdn.datatables.net/1.10.7/js/jquery.dataTables.min.js"></script><script>$(function(){$('.datatable').dataTable( {"order": [[ 0, "desc" ]],"iDisplayLength": -1,"lengthMenu": [[10, 25, 50, -1], [10, 25, 50, "All"]]} );});</script></body></html>